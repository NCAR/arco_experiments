{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21059e0d-3e50-40cf-8e22-d7d909c0d615",
   "metadata": {},
   "source": [
    "# Generate kerchunk sidecar file for Surface Air Temperature data in NetCDF formats (freq: daily and monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20444ba4-275a-45a7-bda5-c22b5cb5cee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# import scipy as sp\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "from kerchunk.hdf import SingleHdf5ToZarr\n",
    "from pathlib import Path\n",
    "import ujson\n",
    "import intake_esm\n",
    "import intake\n",
    "import fsspec\n",
    "import kerchunk.hdf\n",
    "from kerchunk.combine import MultiZarrToZarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9928a424-cf23-4a2f-bf16-afe7137d296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, performance_report\n",
    "from dask_jobqueue import PBSCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa28e3b-ed0a-41b0-8123-2ff3679366f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ujson\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "import kerchunk.hdf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86d2e3a-f4ae-4b84-955d-5c9ee0978619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "rda_scratch = \"/gpfs/csfs1/collections/rda/scratch/harshah\"\n",
    "rda_data    = \"/gpfs/csfs1/collections/rda/data/\"\n",
    "myrda_data  = rda_data + 'harshah/'\n",
    "era5_path   = rda_data + \"ds633.0/e5.oper.an.sfc/\"\n",
    "#\n",
    "pelican_test = '/glade/campaign/collections/rda/transfer/harshah/pelican_test'\n",
    "zarr_path   = rda_scratch + \"/tas_zarr/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6857511-f72d-489e-acaa-af6d1aefdc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_json(file_url, write_json=False):\n",
    "    print(f'generating {file_url}')\n",
    "    with fs.open(file_url, **so) as infile:\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(infile, file_url, inline_threshold=366 )\n",
    "        year = file_url.split('/')[-1].split('.')[0]\n",
    "        file_basename = os.path.basename(file_url)\n",
    "        outfile = f'{file_basename}.json'\n",
    "        if write_json:\n",
    "            with fs.open(outfile, 'wb') as f:\n",
    "                print(f'writing {outfile}')\n",
    "                f.write(ujson.dumps(h5chunks.translate()).encode());\n",
    "        return h5chunks.translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be95d62a-bbe4-4ce3-868a-8942771de1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find NetCDF files with tas (Surface air temperature at 2m) using glob and a search pattern\n",
    "tas_pattern = era5_path + \"**/e5.oper.an.sfc.128_167_2t.*.nc\"\n",
    "tas_ncfiles = glob.glob(tas_pattern, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca2b1405-c2bd-45c8-9f6e-94d626156525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/harshah/conda-envs/arco_experiments/lib/python3.12/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 37213 instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a PBS cluster object\n",
    "cluster = PBSCluster(\n",
    "    job_name = 'dask-wk24-hpc',\n",
    "    cores = 1,\n",
    "    memory = '8GiB',\n",
    "    processes = 1,\n",
    "    local_directory = rda_scratch+'/dask/spill',\n",
    "    log_directory = rda_scratch +'/dask/',\n",
    "    resource_spec = 'select=1:ncpus=1:mem=8GB',\n",
    "    queue = 'casper',\n",
    "    walltime = '2:00:00',\n",
    "    #interface = 'ib0'\n",
    "    interface = 'ext'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "573d806b-2aa4-4453-b535-784ac39a0e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-23ac9682-459c-11ef-8d26-3cecef1aca46</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">f613e6b6</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-7f1c7f92-4dca-462b-b4df-3a631b1eef28</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.96:33717\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/harshah/proxy/37213/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.96:33717' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "311825d7-f498-4438-856a-a38844c3f684",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76003fc-8546-455d-9956-e42277d2c6b9",
   "metadata": {},
   "source": [
    "### Open intake catalog and extract paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b10c28dd-3c11-4985-8825-5642eecf7f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/harshah/conda-envs/arco_experiments/lib/python3.12/site-packages/intake_esm/cat.py:249: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>era5_catalog catalog with 7 dataset(s) from 785068 asset(s)</strong>:</p> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>era_id</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datatype</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_type</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step_type</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table_code</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_code</th>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_name</th>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path</th>\n",
       "      <td>785068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derived_variable</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "catalog = intake.open_esm_datastore(rda_scratch + '/intake_catalogs/posix/era5_catalog.json')\n",
    "catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c9ff2a-3f62-4d7c-b498-04b2531c6821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong>era5_catalog catalog with 1 dataset(s) from 1010 asset(s)</strong>:</p> <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>era_id</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datatype</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level_type</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>step_type</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>table_code</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>param_code</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variable</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_name</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>units</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>frequency</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>path</th>\n",
       "      <td>1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derived_variable</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cat_temp = catalog.search(variable ='VAR_2T')\n",
    "cat_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5e41e11-5ff9-472b-83d6-9dc758f86e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    https://data.rda.ucar.edu/ds633.0/e5.oper.an.s...\n",
       "1    https://data.rda.ucar.edu/ds633.0/e5.oper.an.s...\n",
       "2    https://data.rda.ucar.edu/ds633.0/e5.oper.an.s...\n",
       "3    https://data.rda.ucar.edu/ds633.0/e5.oper.an.s...\n",
       "4    https://data.rda.ucar.edu/ds633.0/e5.oper.an.s...\n",
       "Name: path, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_temp.df['path'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d241633-8c36-4486-ac54-f7650c7c3d3b",
   "metadata": {},
   "source": [
    "### Read these netcdf files and rewrite them with uniform chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5f34c54-50e7-40a7-b264-dd37bf4efcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fs = LocalFileSystem()\n",
    "so = dict(mode='rb', default_fill_cache=False, default_cache_type='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed6ca95d-aaf4-4372-bf1b-205dbca60bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rechunk_netcdf(file_url, output_directory, chunk_size):\n",
    "#     \"\"\"Rechunk a NetCDF file and save it to a new directory with the same file name, removing '#mode=bytes'.\"\"\"\n",
    "#     # Open the dataset with the specified chunk size\n",
    "#     ds = xr.open_dataset(file_url, chunks=chunk_size)\n",
    "    \n",
    "#     # Extract the file name from the file URL and remove '#mode=bytes' if present\n",
    "#     file_name = os.path.basename(file_url).replace('#mode=bytes', '')\n",
    "    \n",
    "#     # Create the output path\n",
    "#     output_path = os.path.join(output_directory, file_name)\n",
    "    \n",
    "#     # Save the rechunked dataset to the new directory\n",
    "#     ds.to_netcdf(output_path)\n",
    "#     print(f'Saved rechunked file to {output_path}')\n",
    "#     return output_path\n",
    "\n",
    "# def process_and_rechunk_files(file_urls, output_directory, chunk_size):\n",
    "#     \"\"\"Process a list of file URLs, rechunk them, and save them to a new directory.\"\"\"\n",
    "#     os.makedirs(output_directory, exist_ok=True)\n",
    "#     rechunked_files = []\n",
    "#     for file_url in file_urls:\n",
    "#         rechunked_file = rechunk_netcdf(file_url, output_directory, chunk_size)\n",
    "#         rechunked_files.append(rechunked_file)\n",
    "#     return rechunked_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0045a281-2abf-47b1-b2a7-8f55e55723c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rechunk_netcdf(file_url, output_directory, chunk_size, overwrite=False):\n",
    "    \"\"\"Rechunk a NetCDF file and save it to a new directory with the same file name, removing '#mode=bytes'.\n",
    "    Args:\n",
    "        file_url (str): URL or path to the input NetCDF file.\n",
    "        output_directory (str): Directory where the rechunked file will be saved.\n",
    "        chunk_size (dict): Dictionary specifying the chunk sizes.\n",
    "        overwrite (bool): Whether to overwrite existing files. Default is False.\n",
    "    \"\"\"\n",
    "    # Extract the file name from the file URL and remove '#mode=bytes' if present\n",
    "    file_name = os.path.basename(file_url).replace('#mode=bytes', '')\n",
    "    \n",
    "    # Create the output path\n",
    "    output_path = os.path.join(output_directory, file_name)\n",
    "    \n",
    "    # Check if the file already exists and overwrite is False\n",
    "    if os.path.exists(output_path) and not overwrite:\n",
    "        print(f'File {output_path} already exists and overwrite is set to False. Skipping.')\n",
    "        return output_path\n",
    "    \n",
    "    # Open the dataset with the specified chunk size\n",
    "    ds = xr.open_dataset(file_url, chunks=chunk_size)\n",
    "    \n",
    "    # Save the rechunked dataset to the new directory\n",
    "    ds.to_netcdf(output_path)\n",
    "    print(f'Saved rechunked file to {output_path}')\n",
    "    return output_path\n",
    "\n",
    "def process_and_rechunk_files(file_urls, output_directory, chunk_size, overwrite=False):\n",
    "    \"\"\"Process a list of file URLs, rechunk them, and save them to a new directory.\n",
    "    Args:\n",
    "        file_urls (list): List of URLs or paths to the input NetCDF files.\n",
    "        output_directory (str): Directory where the rechunked files will be saved.\n",
    "        chunk_size (dict): Dictionary specifying the chunk sizes.\n",
    "        overwrite (bool): Whether to overwrite existing files. Default is False.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    rechunked_files = []\n",
    "    for file_url in file_urls:\n",
    "        rechunked_file = rechunk_netcdf(file_url, output_directory, chunk_size, overwrite)\n",
    "        rechunked_files.append(rechunked_file)\n",
    "    return rechunked_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50921791-f989-4760-a9c9-c7b5f2f8c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_individual_json(file_url, output_directory='.', inline_threshold=366):\n",
    "    \"\"\"Generate individual Kerchunk JSON sidecar file for a given file URL.\"\"\"\n",
    "    if file_url.startswith(\"http\"):\n",
    "        so = dict(mode='rb')\n",
    "    else:\n",
    "        so = dict(mode='rb', default_fill_cache=False, default_cache_type='first')\n",
    "    \n",
    "    with fsspec.open(file_url, **so) as infile:\n",
    "        h5chunks = kerchunk.hdf.SingleHdf5ToZarr(infile, file_url, inline_threshold=inline_threshold)\n",
    "        file_basename = os.path.basename(file_url)\n",
    "        outfile = os.path.join(output_directory, f'{file_basename}.json')\n",
    "        with open(outfile, 'w') as f:\n",
    "            ujson.dump(h5chunks.translate(), f)\n",
    "        return outfile\n",
    "\n",
    "def process_files(file_paths, output_directory='.'):\n",
    "    \"\"\"Process a list of file paths and generate individual JSON sidecar files.\"\"\"\n",
    "    json_files = []\n",
    "    for file_path in file_paths:\n",
    "        json_file = gen_individual_json(file_path, output_directory)\n",
    "        json_files.append(json_file)\n",
    "    return json_files\n",
    "\n",
    "def combine_json_files(json_files, output_path, concat_dim='time'):\n",
    "    \"\"\"Combine individual JSON sidecar files into a single aggregate JSON sidecar file.\"\"\"\n",
    "    refs = [ujson.load(open(f)) for f in json_files]\n",
    "\n",
    "    print('Combining references into a single JSON sidecar file')\n",
    "    mzz = MultiZarrToZarr(\n",
    "        path=json_files,\n",
    "        indicts=refs, \n",
    "        concat_dims=[concat_dim],\n",
    "    )\n",
    "    combined = mzz.translate()\n",
    "    \n",
    "    with open(output_path, 'w') as f:\n",
    "        ujson.dump(combined, f)\n",
    "    print(f'Combined JSON sidecar file written to {output_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c68e3427-0666-43a5-9317-bd75b74917b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23 μs, sys: 5 μs, total: 28 μs\n",
      "Wall time: 31.7 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Example usage in Jupyter Notebook\n",
    "# Assuming `catalog` is your intake catalog and has a DataFrame attribute `df` with a 'path' column\n",
    "\n",
    "cat_temp_df      = cat_temp.df  # This should be your actual DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92bb4a-4f39-4eaf-bc33-e9b861643488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940010100_1940013123.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940020100_1940022923.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940030100_1940033123.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940040100_1940043023.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940050100_1940053123.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940060100_1940063023.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940070100_1940073123.nc already exists and overwrite is set to False. Skipping.\n",
      "File /gpfs/csfs1/collections/rda/data/harshah//era5_tas/e5.oper.an.sfc.128_167_2t.ll025sc.1940080100_1940083123.nc already exists and overwrite is set to False. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/harshah/conda-envs/arco_experiments/lib/python3.12/site-packages/xarray/core/dataset.py:274: UserWarning: The specified chunks separate the stored chunks along dimension \"time\" starting at index 27. This could degrade performance. Instead, consider rechunking after loading.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_directory = myrda_data +'/era5_tas'  # Update to your actual path\n",
    "chunk_size = {\"time\": 27, \"lat\": 139, \"lon\": 277}  # Update with your desired chunk sizes\n",
    "# Rechunk the files and save them to the new directory\n",
    "file_urls = cat_temp_df['path'][:36].tolist()\n",
    "rechunked_files = process_and_rechunk_files(file_urls, output_directory, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06652d6-21a6-438b-8e45-41ff20d64d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_file_paths(file_urls, output_directory):\n",
    "    \"\"\"Create new file paths for the rechunked files.\"\"\"\n",
    "    new_file_paths = []\n",
    "    for file_url in file_urls:\n",
    "        # Extract the file name from the file URL and remove '#mode=bytes'\n",
    "        file_name = os.path.basename(file_url).replace('#mode=bytes', '')\n",
    "        # Create the new file path\n",
    "        new_file_path = os.path.join(output_directory, file_name)\n",
    "        new_file_paths.append(new_file_path)\n",
    "    return new_file_paths\n",
    "\n",
    "# Create new file paths for the rechunked files\n",
    "new_file_paths = create_new_file_paths(file_urls, output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3a2b6-ab7e-4925-86c8-67997bff6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "sidecar_directory = rda_scratch + '/sidecar/era5/'\n",
    "sidecar_path      = sidecar_directory + 'era5_2t_https.json'  # Update to your desired output directory\n",
    "# Generate individual JSON sidecar files\n",
    "json_files = process_files(new_file_paths, sidecar_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7a8c8dcf-e37d-4f66-a3c8-f31f14aff850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining references into a single JSON sidecar file\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found chunk size mismatch:\n                        at prefix VAR_2T in iteration 1 (file /gpfs/csfs1/collections/rda/scratch/harshah/sidecar/era5/e5.oper.an.sfc.128_167_2t.ll025sc.1940050100_1940053123.nc#mode=bytes.json)\n                        new chunk: [27, 139, 279]\n                        chunks so far: [27, 139, 277]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed eval>:2\u001b[0m\n",
      "Cell \u001b[0;32mIn[33], line 34\u001b[0m, in \u001b[0;36mcombine_json_files\u001b[0;34m(json_files, output_path, concat_dim)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombining references into a single JSON sidecar file\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m mzz \u001b[38;5;241m=\u001b[39m MultiZarrToZarr(\n\u001b[1;32m     30\u001b[0m     path\u001b[38;5;241m=\u001b[39mjson_files,\n\u001b[1;32m     31\u001b[0m     indicts\u001b[38;5;241m=\u001b[39mrefs, \n\u001b[1;32m     32\u001b[0m     concat_dims\u001b[38;5;241m=\u001b[39m[concat_dim],\n\u001b[1;32m     33\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mmzz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     37\u001b[0m     ujson\u001b[38;5;241m.\u001b[39mdump(combined, f)\n",
      "File \u001b[0;32m/glade/work/harshah/conda-envs/arco_experiments/lib/python3.12/site-packages/kerchunk/combine.py:604\u001b[0m, in \u001b[0;36mMultiZarrToZarr.translate\u001b[0;34m(self, filename, storage_options)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore_coords()\n\u001b[1;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msecond_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/glade/work/harshah/conda-envs/arco_experiments/lib/python3.12/site-packages/kerchunk/combine.py:503\u001b[0m, in \u001b[0;36mMultiZarrToZarr.second_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m     chunk_sizes[v] \u001b[38;5;241m=\u001b[39m zarray[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m chunk_sizes[v] \u001b[38;5;241m!=\u001b[39m zarray[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mFound chunk size mismatch:\u001b[39m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;124m        at prefix \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_paths[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124m        new chunk: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_sizes[v]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124m        chunks so far: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzarray[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     )\n\u001b[1;32m    509\u001b[0m chunks \u001b[38;5;241m=\u001b[39m chunk_sizes[v]\n\u001b[1;32m    510\u001b[0m zattrs \u001b[38;5;241m=\u001b[39m ujson\u001b[38;5;241m.\u001b[39mloads(m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/.zattrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Found chunk size mismatch:\n                        at prefix VAR_2T in iteration 1 (file /gpfs/csfs1/collections/rda/scratch/harshah/sidecar/era5/e5.oper.an.sfc.128_167_2t.ll025sc.1940050100_1940053123.nc#mode=bytes.json)\n                        new chunk: [27, 139, 279]\n                        chunks so far: [27, 139, 277]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Combine individual JSON sidecar files into a single aggregate JSON sidecar file\n",
    "combine_json_files(json_files, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09130c8-c61a-4104-be6a-09cc33d64050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the combined Kerchunk sidecar file\n",
    "# combined_kerchunk_path =  output_path # Update to your actual path\n",
    "\n",
    "# # Create a fsspec filesystem using the Kerchunk reference file\n",
    "# fs = fsspec.filesystem(\n",
    "#     \"reference\", fo=combined_kerchunk_path, remote_protocol=\"https\"\n",
    "# )\n",
    "\n",
    "# # Get a mapper from the filesystem\n",
    "# m = fs.get_mapper(\"\")\n",
    "\n",
    "# # Open the dataset using xarray\n",
    "# ds = xr.open_dataset(m, engine=\"zarr\", backend_kwargs=dict(consolidated=False))\n",
    "\n",
    "# # Print the dataset\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2704ba1-d8bd-48c7-af73-cce642e6babd",
   "metadata": {},
   "source": [
    "####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c03bc60-d81c-4019-a088-181e319c94a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage in Jupyter Notebook\n",
    "# # Assuming `catalog` is your intake catalog and has a DataFrame attribute `df` with a 'path' column\n",
    "\n",
    "# cat_temp_df = cat_temp.df  # This should be your actual DataFrame\n",
    "# file_paths = cat_temp_df['path'].tolist()\n",
    "# output_directory = rda_scratch + '/sidecar/era5/'  # Update to your desired output directory\n",
    "\n",
    "# # Process the files and generate JSON sidecar files\n",
    "# process_files(file_paths, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe3ed8-c4c1-4b3f-87a2-7143951b5467",
   "metadata": {},
   "source": [
    "## Let us rewrite these files into a zarr store"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:arco_experiments]",
   "language": "python",
   "name": "conda-env-arco_experiments-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
